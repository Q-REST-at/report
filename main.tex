% main.tex

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[most]{tcolorbox}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{booktabs}
\usepackage[numbers,sort&compress]{natbib}

\graphicspath{ {./images/} }

% Define custom colors
% \definecolor{main}{HTML}{5989cf}
% \definecolor{sub}{HTML}{cde4ff}  
% Olive color palette
\definecolor{main}{HTML}{46AF70}
\definecolor{sub}{HTML}{E2F3E9}  
\definecolor{fontColor}{HTML}{2D5B9A}
\definecolor{white}{HTML}{FFFFFF}

% Research Question Box Style
\newtcolorbox{RQBox}{
    colback = sub!50, 
    colframe = main, 
    boxrule = 0pt, 
    leftrule = 6pt
}

% Glossary box style
\newtcolorbox{GlossaryBox}{
    enhanced,
    boxrule = 0pt,
    borderline = {0.75pt}{0pt}{main},
    borderline = {0.75pt}{2pt}{sub},
    colback = white
}


% Experiment Structure Rounded Box Style
\newtcolorbox{roundedBox}{
    fontupper=\footnotesize,
    colback=sub!30,
    boxrule=1.5pt,
    colframe=main,
    rounded corners,
    arc=5pt,
    boxsep=0pt, left=0pt, right=0pt,
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Evaluating Trade-offs of Quantized LLMs \\ for Requirements and Test Alignment}

% Evaluating Trade-offs of Using Quantized LLMs to Generate Trace Links Between Software Artifacts and Requirements In Software Projects That Utilize Traceability to Enhance Software Development --- A Comparative Experimental Empirical Investigative Analysis for the Purpose of Providing Insights

\author{
  \IEEEauthorblockN{Erik Lindstrand}
  \IEEEauthorblockA{\textit{Computer Science and Engineering} \\
  \textit{Chalmers and Gothenburg University}\\
  Gothenburg, Sweden \\
  elindstr@chalmers.se}
  \and
  \IEEEauthorblockN{Mariia Zabolotnia}
  \IEEEauthorblockA{\textit{Computer Science and Engineering} \\
  \textit{Chalmers and Gothenburg University}\\
  Gothenburg, Sweden \\
  mariiaz@chalmers.se}
  \and
  \IEEEauthorblockN{Michal Spano}
  \IEEEauthorblockA{\textit{Computer Science and Engineering} \\
  \textit{Chalmers and Gothenburg University}\\
  Gothenburg, Sweden \\
  spano@chalmers.se}
  \and
  \centering % TODO: actually center this
  Supervisor:
  \IEEEauthorblockN{Francisco Gomes de Oliveira Neto}
  \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
  \textit{name of organization (of Aff.)}\\
  City, Country \\
  email address or ORCID}
}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have shown impressive capabilities (e.g., in terms of content generation, language processing) in the scientific fields of computing, software engineering, and now yield promising results in REST initiatives. However, the larger, and thus more performant, a model becomes, the more resources are required for its proper operation. As the systems we develop evolve and take in more functionality over time, associated costs grow rapidly. Fortunately, previous research discusses compression techniques, notably quantization, to decrease both training and operating costs. In this paper, we aim to investigate the impact of quantization of LLMs for generating trace links between requirements and test case artifacts. To achieve this, we conduct an experiment evaluating the rates of alignment given numerous models and their quantized counterparts. Our contributions aim to evaluate the feasibility of quantized LLMs in REST applications, focusing on efficacy, efficiency, and practical heuristics for industrial implementation.
\end{abstract}

\begin{IEEEkeywords}
Large Language Models, REST, Traceability, Quantization, Software Testing
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}

The modeling of human language---a long-established field within science---has particularly seen its rise with the introduction of web- and transformer-based chatbots (e.g. ChatGPT, DeepSeek) \cite{jones1994Natural,vaswani2017Attention,naveed2024Comprehensive}. Since then, science, education, and medicine have all made use of LLMs that employ the transformer architecture. Moreover, LLMs have been exceptionally helpful in Software Engineering tasks \cite{naveed2024Comprehensive}; notably in requirements engineering \cite{arora2024Advancing} or software testing \cite{dakhel2024Effective, wang2024Software}. In particular, Ivarsson and Setterstr√∂m have demonstrated that LLM-assisted trace link generation is feasible and yields satisfactory results \cite{ivarsson2023automated}. This claim is further supported by a study of Quinstedt and Lindgren who developed the \verb|REST-at| tool that this study builds upon \cite{quinstedt2024Optimizing}. 

Given the growing size and complexity of software systems, the coordination of work and artifacts is oftentimes challenging. As a result, researchers and practitioners identify \textbf{trace links} between software and development artifacts to manage this complexity \cite{jaber2013Effect}.

Research and development in LLMs are advancing rapidly, along with their performance capabilities. Typically, a more performant model requires more computational resources due to an increased number of model parameters, leading to financial and environmental concerns \cite{naveed2024Comprehensive}. Fortunately, there exist well-established strategies to mitigate the hardware requirements necessary to load and run LLMs, and one that particularly interests us is \textbf{quantization} (c.f. \cite{shen2024exploring, frantar2023GPTQ, lin2024AWQ, chen2024EfficientQAT}). 

However, given the exponential growth in terms of cost, there is an incentive to improve the efficiency of LLM tools, e.g. through quantization methods, to better utilize automated trace generation tools at scale. Therefore, the purpose of this study is to evaluate the \textbf{efficacy} of quantized models when used to generate REST trace links. Furthermore, the trade-off analysis will consider the \textbf{efficiency} of the models, which is a consequence of model size and hence affected by the quantization that is applied.

\begin{RQBox}
    \textbf{RQ1}: How do quantized LLMs perform compared to non-quantized LLMs
    in terms of \textbf{efficacy} when used to create REST trace links?\\[0.5em]
    We will evaluate model efficacy using the following metrics: \textit{accuracy, precision, recall, F1-score}
\end{RQBox}


\begin{RQBox}
    \textbf{RQ2}: How do quantized LLMs perform compared to non-quantized LLMs in
    terms of \textbf{efficiency} when used to create REST trace links? \\[0.5em]
    We will evaluate model efficiency using the following metrics: \textit{time-to-analyze, GPU memory usage (vRAM)}.
\end{RQBox}

\begin{RQBox}
    \textbf{RQ3}: What are the trade-offs of using quantized LLMs for REST alignment
    with respect to efficacy versus efficiency? \\[0.5em]
    We are particularly interested in investigating whether it is possible to balance acceptable recall and F1-score with lowered memory usage.
\end{RQBox}


% Encapsulated footnote for reusing in several places 
\newcommand{\modelsFootnote}{In this paper, a ``baseline'' model is one of the following: Mistral 7B Instruct-v0.2 (i.e., Mistral), Mixtral 8x7B Instruct-v0.1 (i.e., Mixtral), and LLAMA 3 70B-Instruct (i.e., LLAMA 3).} 

Our findings arise from an experiment where we compare the generated trace links from each model against the \textit{ground truth}\footnote{This essentially stands for the desired mapping of requirements and tests, established by the respective company.} provided by our industry partner TestScouts\footnote{\url{https://testscouts.se/}}. We generate trace links with each model using the \verb|REST-at| tool which we run on \verb|Alvis|, a cloud platform for scientific computing\footnote{\url{https://www.c3se.chalmers.se/about/Alvis}}. We access both the baseline\footnote{\label{baselineModels}\modelsFootnote} and quantized models from public repositories hosted on \textit{HuggingFace}\footnote{\url{https://huggingface.co/}}.

Our scientific contributions include: evaluating the feasibility of quantized LLMs for REST in terms of efficacy and efficiency metrics; examining the trade-offs, such as the practicality of use and how efficacy is affected when optimizing for lower operational costs. Furthermore, our technical contributions include revising REST-at to support quantized models as well as updating the tool to comply with other technological changes that are affecting its use and have been introduced since it was developed. Additionally, we aim to provide guidelines for implementing quantization in industrial REST applications.

\section{Background \& Related Work}\label{background}

\subsection{Requirements Engineering and System Testing Alignment}

REST (Requirements Engineering and System Testing) alignment has been the subject of prior research studies, as has been shown in systematic literature mappings \cite{barmi2011Alignment, karhaapa2017What}. Achieving REST alignment involves activities that coordinate Requirements Engineering and System Testing efforts in order to optimize product development \cite{unterkalmsteiner2014Taxonomy}. Traceability is one of the tools that can be used to achieve alignment through the structuring of artifacts, such as requirement specifications and test cases, by creating connections (or traces) that help to evaluate and improve requirements coverage \cite{bjarnason2014Challenges}. Jaber et al. define a trace link as any link between different artifacts, such as a particular \textit{code element} (e.g., software test) in relation to a \textit{design element} (e.g., requirement specification) \cite{jaber2013Effect}. 

Real-world challenges in aligning RE and ST practices have been examined in previous case studies \cite{bjarnason2014Challenges,gomes2017Challenges}. According to both Bjarnason et al. and Gomes et al., introducing tracing between requirements and test cases is costly, meanwhile, a lack of traceability also comes with significant additional cost. There is also significant challenge in updating and maintaining traces between RE and ST artifacts, e.g., when requirements change. Moreover, the case study by Bjarnason et al. identifies a need, among the companies involved in the study, for tools that can manage REST traceability artifacts \cite{bjarnason2014Challenges}.

\subsection{Automated REST Alignment Using LLMs} 

There have been studies on the development of more advanced tools to aid in the trace creation process, with Ivarsson and Setterstr√∂m showing promise in automating the process using LLMs by leveraging OpenAI‚Äôs GPT-3.5-turbo model\cite{ivarsson2023automated}. Their tool achieved an average of 86.394\% across accuracy and recall, although with limitations in terms of precision with an average of 45.582\%. Notably, the computational complexity is nonlinear in relation to the input size (requirements + test cases), resulting in an exponentially growing cost both financially and in \textit{time-to-analyze}, i.e., returning the results (trace links), which negatively impacts scalability \cite{ivarsson2023automated}. Furthermore, the study by Ivarsson and Setterstr√∂m included identifying the main requirements for an automated REST tracing tool by performing literature reviews as well as conducting interviews with practitioners working at TestScouts, a company specializing in software testing \cite{ivarsson2023automated}.

Building on the work of Ivarsson and Setterstr√∂m, Quinstedt and Lindgren conducted a study in which they evaluated and compared the efficiency of several different LLMs in creating REST trace links \cite{quinstedt2024Optimizing}. The requirements identified in Ivarsson and Setterstr√∂m's study \cite{ivarsson2023automated} were further refined in collaboration with the same industry partner, TestScouts. These requirements were then used to develop a tool called \textit{REST-at} (REST alignment tool) which is capable of creating REST trace links by interfacing with proprietary models, such as OpenAI's GPT-4, through the use of APIs, in addition to using non-proprietary locally stored LLMs. Importantly, their study showed that smaller LLMs (in terms of number of parameters) were capable of achieving comparable results with much larger LLMs, with LLAMA 3 (70B parameters) achieving a 100\% recall score on the GBGE dataset, compared to GPT-3.5 (175B parameters) which had a recall score of 90.87\%.

However, Quinstedt and Lindgren's study also identified a key challenge in regards to hardware requirements and cost, due to the fact that running the LLMs require substantial computational resources. For instance, running the previously mentioned LLAMA 3 model with its 70B parameters requires 140 GB vRAM \cite{quinstedt2024Optimizing}. In order to mitigate this challenge, the study had to be adapted to instead run the models on the Alvis research cloud  infrastructure, offering significantly more computational power.

Given the challenges faced due to hardware constraints when utilizing LLMs, there is a need to explore techniques that would reduce the GPU memory usage. Moreover, it is still unclear whether using quantized models is useful for the purposes of REST alignment. Hence, we identify a gap in the current research regarding the application of quantized LLMs for the purpose of REST alignment. Thus, we will explore LLM quantization methods as a viable technique to allow for models to run in a non-cloud environment while retaining reasonable performance in terms of automating REST traceability.

\subsection{Quantization of LLMs}

Quantization is one of the LLM compression techniques, along with pruning, knowledge distillation, and low-rank approximation\cite{bai2024beyond}. It stands out for its ability to directly reduce memory usage while maintaining reasonable accuracy and performance of LLMs. Some studies even report a notable acceleration of the inference latency for quantized models\cite{shen2024exploring}.

Drawing a parallel to common image compression principles (cf. Figure \ref{fig:quantvisual}), quantization is performed, in essence, by converting the weights and activations from a high-precision data representation to a reduced-precision format \cite{zhao2025benchmarking, bai2024beyond}. This is illustrated by transforming the FP32(32 bits) number to, for example, INT8(8 bits), which is particularly impactful at the scale of billion parameters, enabling smaller-size model execution on resource-constrained devices.

\begin{figure}[ht]
    \includegraphics[width=\columnwidth]{img}
    \caption{Analogy between color quantization and LLM quantization}
    \label{fig:quantvisual}
\end{figure}

Quantization can be applied 1) during the training phase (\textit{QAT}) which is observed to be highly efficient; however, due to significant resource demands, oftentimes appears impractical \cite{chen2024EfficientQAT}, and 2) post-training (\textit{PTQ}), which is more resource efficient, but can lead to certain levels of performance degradation \cite{shen2024exploring}. An important quantitative marker of precision loss, quantization error, is often used to assses the degree of approximation loss \cite{lin2024AWQ}.

Having established the theoretical foundation of quantization, we now provide an overview of key methods. \textit{GPTQ} (Generative Pre-trained Transformers Quantization) focuses solely on weights and aims to minimize quantization error through optimal weight rounding. It is almost twice as effective as its one-shot predecessors and is capable of quantizing a 175 billion parameter model in a few hours with minimal accuracy loss\cite{frantar2023GPTQ}. \textit{AWQ} (Activation-aware Weight Quantization) assumes that weights carry varying levels of importance; therefore skipping crucial activation outliers while aggressively quantizing the rest helps mitigate accuracy loss. It also shows approximately 3x faster inference speed on a desktop, even allowing deployment and execution of a 13-billion model on a laptop with 8GB of RAM, which is of high interest for our study \cite{lin2024AWQ}. \textit{GGUF} (GPT-Generated Unified Format), in turn a successor of the deprecated \textit{GGML} (GPT-Generated Model Language), includes  quantization-aware kernels optimizations and  provides backward-compatible file format among other improvements. By offering such features as single-file deployment and improved loading and saving speed, it introduces a smoother workflow for forking with LLMs \cite{rajput2024benchmarking}. \textit{SmoothQuant} \cite{xiao2023SmoothQuant} enables quantization for both activations and weights by managing outlier values and QLoRA (4-bit quantized version of LoRA fine-tuning technique) \cite{dettmers2023qlora} reduces GPU requirements while still preserving performance.

% The described approaches demonstrate not only superior advancements and efficiency, but are also feasible for us to experiment with. Therefore, in this study, we plan to quantize each of the baseline models\footnotemark[\getrefnumber{baselineModels}] using the GPTQ, GGUF and AWQ techniques to reduce model size \footnote{By ``size'' we refer to bit width which in turn affects the needed memory for execution} to mitigate hardware requirements. Later we employ them to generate REST trace links, analyze their performance, and answer our RQs.

The described approaches demonstrate not only superior advancements and efficiency, but are also feasible for us to experiment with. Therefore, in this study, we plan to access models from HuggingFace that have been quantized using the following techniques: GPTQ, GGUF and AWQ. These approaches reduce model size\footnote{By ``size'' we refer to \textit{bit width} which in turn affects the memory needed for execution} to mitigate hardware requirements. Later we employ them to generate REST trace links, analyze their performance, and answer our RQs. 

\section{Research Methodology}\label{sec:method}

% TODO: Mention something here about insecure quantization (see Exploiting quantization). -> WHY we consider quantization on our own.

This section outlines the research type and its design (cf. Figure \ref{fig:method-overview}), and defines the scope pertaining to identified RQs. We structure our methodology section following the experimental process recommended by Wohlin et al. \cite{wohlin2012experimentation}. Furthermore, we explain and motivate the choice of methods and statistical tests for data analysis. Lastly, we acknowledge identified validity threats accompanied by mitigation strategies.

We perform an \textit{experiment}, a research method commonly used to explore empirical correlations between several factors, in our case, LLM models and quantization techniques. Given the nature of  experiments, we gain control over subjects, objects, and instrumentation in order to operate on experimental units and draw conclusions on dependent variable output. Experiments are conducted to test the hypotheses and comparatively assess the impact of specific variables in a controlled setting, which is the most suitable setup for answering our RQs.

\begin{figure}[h]
    \centering
    \includesvg[width=\columnwidth]{method-overview-refined.svg}
    \caption{Overview of the research methodology process}
    \label{fig:method-overview}
\end{figure}

\subsection{Scope}
We define the scope of our experiment using the said framework of Wohlin et al. \cite{wohlin2012experimentation}. That is, we analyze quantized LLMs for the purpose of evaluation with respect to their efficacy, efficiency, and practicality from the point of view of developers and/or managers in the context of industry REST alignment initiatives. A more concrete realization of the scope is described in the following paragraphs.

\subsection{Planning}

We will conduct a full factorial controlled experiment. There are two factors in our study: baseline models\footnotemark[\getrefnumber{baselineModels}] and quantization approaches. There are three levels for the models: LLama, Mistral, and Mixtral; and four levels for the quantization: \textit{None}, GPTQ, GGUF and AWQ. We generate $3 \times 4 = 12$ treatments. Our choice of models is based on the previous work of Quinstedt and Lindgren \cite{quinstedt2024Optimizing} as they opened up the questions explored in our study; thus aligning the model selection with their study allows for a more detailed and direct trade-off comparison.

The dependent variables reflect the definition of performance in this study; they are: accuracy, precision, recall, and F1-score (RQ1, efficacy), as well as time-to-analyze and GPU memory-usage (RQ2, efficiency).

Our control variables include requirements specification and test case files, as well as the ground truth which specifies the correct trace links between tests and requirements. This data comes from two datasets: the first has been provided to us by our industry partner and originates from G\"oteborg Energi, the second was sourced from Bluetooth Headset Profile 1.2\footnote{\url{https://www.bluetooth.com/specifications/specs/headset-profile-1-2/}}. Additionally, our control variables include a prompt template which will be used with the REST-at tool and is verified in the previous study by Quinstedt and Lindgren \cite{quinstedt2024Optimizing}). They found that various prompt templates lead to fluctuations in the results; therefore we have selected the most performant one. Furthermore, model hyper-parameters will be kept constant and configured to be as similar as possible between different models, such as model \textit{temperature} that controls the randomness of an LLM's output. We also standardize the Python environment (version) as it has a direct impact on code execution speeds and therefore also on our results.

An overview of the experimental design can be found in Figure \ref{fig:exp-design}. See Section \ref{sec:instrumentation} for a further specification of the experiment objects.

%%%%%%%%%%%%%%%%%% HYPOTHESIS TABLE %%%%%%%%%%%%%%%%%%

\newcommand{\equalM}{$M_{1} = \dots = M_{12}$}
\newcommand{\notEqualM}{$M_{1} \neq \dots \neq M_{12}$}

\begin{table}[h]
    \centering
    \caption{Experiment Hypotheses}
    \renewcommand{\arraystretch}{1.65} % original value 1.5
    \begin{tabular}{@{} c p{3.5cm} p{3.5cm} @{}}
    \toprule
    &\multicolumn{1}{c}{\textbf{(i) Efficacy RQ1}}
    &\multicolumn{1}{c}{\textbf{(ii) Efficiency RQ2}} \\  
    \midrule
    $h_{0}$ % <-- NULL HYPOTHESIS %
    & There is no difference in \textbf{efficacy} between the treatments
    under test; \equalM.
    & There is no difference in \textbf{efficiency} between the treatments
    under test; \equalM.\\
    $h_{a}$ % <-- ALT HYPOTHESIS %
    & There is a difference in \textbf{efficacy} between the treatments
    under test; \notEqualM.
    & There is a difference in \textbf{efficiency} between the treatments
    under test; \notEqualM.\\
    \bottomrule
    &\multicolumn{2}{c}{Note: the individual treatments are labeled as $M_{1\dots12}$.} \\
    \end{tabular}
    \label{tab:hypothesis}
\end{table}

First, we will collect data and derive metrics from repeated test iterations. The process is repeated on both datasets with 10 repetitions per treatment; we altogether obtain $2 \times 12 \times 10 = 240$ artifacts containing trace links. When analyzing the data, we will initially examine the treatments from two perspectives: (i) efficacy, and (ii) efficiency---evaluating the null hypotheses for these groups of metrics separately (cf. Table \ref{tab:hypothesis}). After testing for our null and alternative hypotheses, we apply a post-hoc pairwise comparative analysis of the treatment pairs. Finally, a descriptive analysis will investigate trade-offs of using quantized LLMs for REST alignment: we will combine the results from both efficacy and efficiency metrics, as well as some additional factors (e.g., implementation challenges, learning curve, etc.). 

%%%%%%%%%%%%%%%%%% EXPERIMENTAL DESIGN FIGURE %%%%%%%%%%%%%%%%%%

\begin{figure}[h]
\begin{center}
    \begin{tcbraster}[raster columns=2, raster column skip=5pt, raster equal height=rows, raster row skip=5pt]
        \begin{roundedBox}
            \centering
            \textbf{Independent Variables \& Levels}
            \begin{itemize}
                \item Model:
                \begin{itemize}
                    \item LLama
                    \item Mistral
                    \item Mixtral
                \end{itemize}
                \item Quantization technique:
                \begin{itemize}
                    \item None
                    \item GPTQ Quantization
                    \item GGUF Quantization
                    \item AWQ Quantization
                \end{itemize}
            \end{itemize}
        \end{roundedBox}
        \begin{roundedBox}
            \centering
            \textbf{Objects}
            \begin{itemize}
                \item Hardware (Alvis)
                \item Alvis job-scripts
                \item REST-at tool
                % \item Hardware: local
            \end{itemize}
        \end{roundedBox}
        \begin{roundedBox}
            \centering
            \textbf{Dependent Variables}
            \begin{itemize}
                \item Accuracy
                \item Precision
                \item Recall
                \item $F1$-score
                \item Time-to-analyze
                \item GPU memory-usage (vRAM)
            \end{itemize}
        \end{roundedBox}
        \begin{roundedBox}
            \centering 
            \textbf{Control Variables}
            \begin{itemize}
                \item Requirements file
                \item Test case file
                \item Ground truth file
                \item Prompt template
                \item Model hyper-parameters
                \item Python environment (version)
            \end{itemize}
        \end{roundedBox}
        \end{tcbraster}
        \begin{roundedBox}
            \centering
            \textbf{Output (artifacts)}
            \begin{itemize}
            \centering
                \item Structured trace links
            \end{itemize}
        \end{roundedBox}
    \caption{Overview of the experimental design}
    \label{fig:exp-design}
\end{center}
\end{figure}

\subsubsection{Instrumentation}\label{sec:instrumentation}
For our instrumentation, we extend REST-at by adding support for (i) quantized models, and (ii) logging efficiency data from the models' execution. We run our experiment on Alvis, a cloud platform for scientific computing. The system is built around multiple NVIDIA GPUs, and allows executing jobs on demand. Therefore, we execute the experiments on a dedicated set of available NVIDIA A100 Tensor Core GPUs\footnote{\url{https://www.nvidia.com/en-us/data-center/a100/}}, which inherently makes all performance and efficiency metrics rely on this hardware configuration.

For our experiment, we require a set of requirements linked with a corresponding set of tests. The datasets with trace links serve as \textit{ground truth} since these are established by the authors from the companies. In addition, we plan to search for more datasets in \textit{Zenodo}\footnote{\url{https://zenodo.org}} from existing research and open-source projects. 

\subsubsection{Sampling and Data Collection}
This sampling is performed to reduce overall computational costs. The requirement and software test pairs are sampled from each dataset to create subsets ranging from 75 to 80 items using PRNG\footnote{Pseudo-random Number Generator, cf. \url{https://epubs.siam.org/doi/abs/10.1137/0215025}}. No human subjects are involved in this phase beyond the authors of this paper, who oversee the experiments in the event of technical difficulties.

Note that, given the stochastic nature of LLMs, it is important that we perform repeated tests on the same input for each of the treatments in order to include standard deviation measurements across the different metrics; for this reason, we will perform 10 repetitions for each treatment. This will allow us to reason about the consistency of each treatment as well as increase the validity of our measurements and subsequently the results of our analysis.

\subsection{Analysis and Interpretation}

% - Describe and motivate the methods for analysing the data:
%  - Describe any quantitative data analysis that will be performed, if applicable.
%  ‚Äì Describe the types of descriptive statistics you will use (e.g., data visualization, mean)
%  ‚Äì This includes the types of statistical tests
%  and analysis you will apply and how you will interpret the results
% ‚Äì Motivate the selection of statistical tests.

The data analysis includes confusion matrix performance indicators: accuracy, precision, recall, F1-score, to be able to reason about the efficacy of the treatment under test. We derive the mean, median, and standard deviation from the (repeated) tests for each treatment to give reliable measurements for all the different metrics. 

Our experiment is full factorial and includes two factors. The data is paired---as we will be comparing the same data points (rows) between different treatments. First, we will analyze the distribution of the collected data. If the necessary assumptions are met, we will use a parametric two-way ANOVA statistical test. Should the assumptions of ANOVA not hold, we will instead use a non-parametric Friedman test---an alternative to repeated ANOVA-measures \cite{mccrum2008statisticalTests}.

%First, we analyze the collected data using the Kruskal-Wallis statistical test: collected data is non-parametric---there are no assumptions about its distribution---and paired---we are comparing the same data points (rows) between different treatments \cite{wohlin2012experimentation}. % RIP IN PIECE MY SWEET PRINCE ;_; KRUSKWALL 1952-2025

Should the null hypothesis be rejected, indicating that there is a significant difference between at least one of the treatment pairs, we conduct a pairwise post-hoc analysis to identify the differing pairs as well as in which direction they differ. By evaluating the treatment pairs, we can examine whether certain quantization techniques result in better model performance within the given context. 
%The z-score based Dunn's test will be used for the post-hoc analysis, as the sample size is $>$30 \cite{dunn1964dunnTest}. 
% Can also mention that it is a rank-based follow-up test to Kruskal-Wallis - if we can find a source ... -_-
% ...or the Conover-Iman test \cite{conover1979conoverImanTest} will be used depending on the sample size. 
% NOTE: keeping this here since we might use this test instead (it has better statistical power)? Also, is it okay that we cite the original sources here and not literature that recommends the test's usage?
We use the Holm‚ÄìBonferroni method to mitigate \textit{alpha inflation}---that would otherwise increase the risk of type I errors, while also reducing the risk of type II errors compared to the regular Bonferroni method \cite{abdi2010HolmBonferroni}.
Note that we will decide the appropriate post-hoc test method at a later stage and update this section accordingly.

We conduct a descriptive statistical analysis of the results to analyze and discuss the trade-offs of applying quantization to LLMs within the given context. We are particularly interested in examining whether there exists any treatment(s) that retains acceptable / usable recall and F1-score results despite significantly reducing GPU memory-usage. Thus, we combine the efficacy and efficiency metrics alongside the following additional factors: implementation challenges, learning curve, model vRAM usage compared with commercial GPU alternatives, cost comparison of manual and automatic solutions. 

We choose to represent the data in tables comparing the results from the different treatments. For this purpose, \textit{mean $\pm$ standard deviation} is displayed for each of the relevant metrics. We also select box-plots in order to visualize the distribution as well as any outliers in each treatment's performance. Lastly, we use bar charts to give a clear comparison of the performance as a result of the different quantization techniques.

\subsection{Threats to Validity}
% - Describe internal validity threats and mitigations.
% - Describe construct validity threats and mitigations.
% - Describe external validity threats and mitigations.
% - If applicable for your research method, describe further validity threats and mitigations under further categories.

Based on the guidelines proposed by Wohlin et al. \cite{wohlin2012experimentation} we identify the following threats to validity that might have influenced our research findings: 
\subsection*{\textbf{1) Internal validity}}
    \textit{Response variability}: due to stochastic behavior of LLMs, the same prompt can result in different outputs when executed several times; we mitigate this risk by invoking prompts multiple times and defining consistent hyperparameters.

    \textit{Selection of models}: in this study, we select open-weight, state-of-the-art LLMs representative of a common choice among practitioners. This selection also leverages on the previous research by Quinstedt and Lindgren \cite{quinstedt2024Optimizing}, which we are building on. Aligning the selection of LLMs with their results allows for a more extensive and direct trade-off comparison. More models can be explored in future work, and we provide instrumentation in the Methodology section to facilitate the smooth integration.

    \textit{Quantized LLM quality}: we have utilized quantized models distributed via the \textit{HuggingFace} platform. While there is a risk in depending on quantized models from a third-party service, HuggingFace is one of the main sources for developers or organizations to download and/or deploy open weight models. As such, these models are likely to be used by practitioners, meaning that the results of our study are representative of the real-world use of quantized LLMs, given that this assumption holds.

\subsection*{\textbf{2) External validity}}
    
    \textit{Domain-Specific Applicability}: our study evaluates quantized LLMs within the specific use case of REST alignment. Exploring the trade-off of quantization in using LLMs for other Software Engineering tasks is beyond the scope of our thesis; however, it could be explored in future work.

\subsection*{\textbf{3) Construct validity}}

\textit{Ground Truth Validity}: the REST alignment mappings that are used in this study as ground truth have been obtained from our Industry Partner (specializing in software testing) and are representative of software artifacts in industry. However, any potential human bias, error, or other factor influencing the creation of these trace links could have an impact on the results of this study/experiment.

\textit{The need for more datasets}: in this study we are using two datasets to evaluate the tradeoffs of using quantization. However, incorporating more datasets could provide a broader perspective and improved generalization of the results, opening up new opportunities for future research to validate obtained results across different contexts.

\section*{Acknowledgment}
We dedicate our gratitude to our supervisor Francisco Gomes de Oliveira Neto and responsible for the course in Research methods in Software Engineering, Birgit Penzenstadler, for their valuable insights. Moreover, we are thankful for the computing platform, Alvis, offered by the Chalmers Centre for Computational Science and Engineering.

% =*= DIT832 STUFF =*=
% \section*{Assignment 2: Work Delegation}
% 
% \begin{itemize}
%     \item Erik: \ref{background}, \ref{relatedWork} A, \ref{method} C
%     \item Mariia: Revised Quantization of LLMs \ref{relatedWork} B; Methodology Overview \ref{method} A, Threats to Validity \ref{method} D 
%     \item Michal: Revised \ref{intro}, Abstract; Data Collection \ref{method}
% \end{itemize}
% 
% \noindent
% Additionally, all members of the group collectively participated in literature
% review, proof-reading, refining research-questions, designing the experiment, and other tasks.
% 
% \subsection*{Changes since Assignment 1}
% 
% \noindent
% Because the team members are continually getting more familiar with the research
% and its objective, a lot of the text that was previously written is now,
% unfortunately, obsolete (especially Introduction \ref{intro}). Highlighting
% individual changes would be excessive and clutter the document. Instead we
% highlight what has \underline{not} been changed:
% 
% \begin{itemize}
%     \item Background \ref{background}
% \end{itemize}

% =*==*==*==*==*=

% Subsection: Results(?)
% We reassess the existing tool and make minor modifications to ensure compliance
% with the latest framework requirements and improve code readiness. This updated
% version is referred to as ``revised'' \verb|REST-at|\footnote{This version of
% the tool can be accessed via \url{https://github.com/SEM25-BSc/REST-at}.
% A complete list of changes is noted below, cf. \ref{method}.}.
% Subsection: Discussion / Results
%
% Write about Scalability? How does it perform with 10, 20, 50, 100, entries?
%
% MENTION QUANTIZATION STRICTNESS -if we pair wise the base-model with the
% quantized models, assuming the quant models have more "aggressive" quantization
% applied, we could identify the "drop-off" point, i.e., when the quantized models
% start to significantly perform worse --- useful if they perform equally well up
% until some point 

% =*= USE LATER SECTION =*=

\bibliographystyle{IEEEtran}
\bibliography{lib}

% FIXME: appendix should NOT follow double-column.
\section*{Appendix}\label{appendix}

\textit{(Left intentionally blank)}

\end{document}